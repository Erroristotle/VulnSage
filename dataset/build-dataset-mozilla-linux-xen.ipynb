{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository paths: {'mozilla': PosixPath('/home/azibaeir/Research/Benchmarking/gecko-dev'), 'xen': PosixPath('/home/azibaeir/Research/Benchmarking/xen'), 'linux': PosixPath('/home/azibaeir/Research/Benchmarking/linux')}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import subprocess\n",
    "import re\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "# Global variables\n",
    "BASE_PATH = Path.cwd().parent.parent\n",
    "REPO_PATHS = {\n",
    "    'mozilla': (BASE_PATH / \"gecko-dev\").resolve(),\n",
    "    'xen': (BASE_PATH / \"xen\").resolve(),\n",
    "    'linux': (BASE_PATH / \"linux\").resolve()\n",
    "}\n",
    "DB_PATH = 'database.sqlite'\n",
    "\n",
    "print(f\"Repository paths: {REPO_PATHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_database():\n",
    "    try:\n",
    "        # Check if raw dataset exists\n",
    "        raw_data_path = 'raw-dataset-merged-class-20211022013122.csv'\n",
    "        if not os.path.exists(raw_data_path):\n",
    "            raise FileNotFoundError(f\"Raw dataset file not found: {raw_data_path}\")\n",
    "\n",
    "        print(\"Loading raw dataset...\")\n",
    "        raw_data = pd.read_csv(raw_data_path)\n",
    "        \n",
    "        # Filter data by R_ID for each project\n",
    "        project_data = {\n",
    "            'mozilla': raw_data[raw_data['R_ID'] == 1],\n",
    "            'xen': raw_data[raw_data['R_ID'] == 3],\n",
    "            'linux': raw_data[raw_data['R_ID'] == 2]\n",
    "        }\n",
    "\n",
    "        # Print dataset statistics\n",
    "        for project, df in project_data.items():\n",
    "            print(f\"{project.capitalize()} vulnerabilities: {len(df)}\")\n",
    "\n",
    "        # Create and connect to database\n",
    "        DB_PATH = 'database.sqlite'\n",
    "        if os.path.exists(DB_PATH):\n",
    "            print(f\"Warning: {DB_PATH} already exists. Removing existing file...\")\n",
    "            os.remove(DB_PATH)\n",
    "\n",
    "        print(f\"Creating new database: {DB_PATH}\")\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Define schema\n",
    "        existing_fields = [\n",
    "            \"COMMIT_HASH\", \"VULNERABILITY_CVE\", \"VULNERABILITY_YEAR\", \"VULNERABILITY_CWE\",\n",
    "            \"VULNERABILITY_CATEGORY\"\n",
    "        ]\n",
    "        future_fields = {\n",
    "            \"DESCRIPTION_IN_PATCH\": None,\n",
    "            \"VULNERABLE_CODE_BLOCK\": None,\n",
    "            \"PATCHED_CODE_BLOCK\": None,\n",
    "            \"NUM_FILES_CHANGED\": None,\n",
    "            \"NUM_FUNCTIONS_CHANGED\": None,\n",
    "            \"NUM_LINES_ADDED\": None,\n",
    "            \"NUM_LINES_DELETED\": None\n",
    "        }\n",
    "        all_fields = existing_fields + list(future_fields.keys())\n",
    "\n",
    "        # Create table\n",
    "        print(\"Creating vulnerabilities table...\")\n",
    "        cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS vulnerabilities (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            {', '.join([f\"{field} TEXT\" for field in all_fields])},\n",
    "            PROJECT TEXT\n",
    "        )\n",
    "        ''')\n",
    "\n",
    "        # Insert data for each project\n",
    "        print(\"Inserting vulnerability data...\")\n",
    "        for project, df in project_data.items():\n",
    "            df = df[existing_fields]  # Select only the specified columns\n",
    "            print(f\"Processing {project} data: {len(df)} entries\")\n",
    "            for _, row in df.iterrows():\n",
    "                cursor.execute('''\n",
    "                INSERT INTO vulnerabilities \n",
    "                (COMMIT_HASH, VULNERABILITY_CVE, VULNERABILITY_YEAR, VULNERABILITY_CWE, VULNERABILITY_CATEGORY, PROJECT)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "                ''', (row['COMMIT_HASH'], row['VULNERABILITY_CVE'], row['VULNERABILITY_YEAR'], \n",
    "                      row['VULNERABILITY_CWE'], row['VULNERABILITY_CATEGORY'], project))\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Initial data insertion completed\")\n",
    "\n",
    "        # Update CWE values\n",
    "        print(\"Updating CWE values...\")\n",
    "        cursor.execute(\"SELECT DISTINCT VULNERABILITY_CWE FROM vulnerabilities\")\n",
    "        cwe_values = cursor.fetchall()\n",
    "        \n",
    "        update_count = 0\n",
    "        for cwe_value in cwe_values:\n",
    "            if cwe_value[0]:\n",
    "                cwe_str = str(int(float(cwe_value[0])))\n",
    "                updated_cwe = f\"CWE-{cwe_str}\"\n",
    "                cursor.execute(\"\"\"\n",
    "                    UPDATE vulnerabilities\n",
    "                    SET VULNERABILITY_CWE = ?\n",
    "                    WHERE VULNERABILITY_CWE = ?\n",
    "                \"\"\", (updated_cwe, cwe_value[0]))\n",
    "                update_count += cursor.rowcount\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Updated {update_count} CWE values\")\n",
    "\n",
    "        # Verify database creation\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM vulnerabilities\")\n",
    "        total_count = cursor.fetchone()[0]\n",
    "        print(f\"\\nDatabase creation completed successfully\")\n",
    "        print(f\"Total vulnerabilities in database: {total_count}\")\n",
    "        \n",
    "        # Print count by project\n",
    "        cursor.execute(\"SELECT PROJECT, COUNT(*) FROM vulnerabilities GROUP BY PROJECT\")\n",
    "        for project, count in cursor.fetchall():\n",
    "            print(f\"{project.capitalize()} vulnerabilities: {count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating database: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicates...\n",
      "Found 1293 commit hashes with duplicates\n",
      "\n",
      "Example duplicate entries:\n",
      "COMMIT_HASH: 0031c41be5c529f8329e327b63cde92ba1284842, Occurrences: 106\n",
      "COMMIT_HASH: 00465dba630d390e124f7bd135d708ab1f50908d, Occurrences: 39\n",
      "COMMIT_HASH: 004dd20c078c42719f22f92c0d4356c240ff7595, Occurrences: 6\n",
      "\n",
      "Initial total entries: 19220\n",
      "\n",
      "Counts by project after removing duplicates:\n",
      "Mozilla: 1177\n",
      "Linux: 363\n",
      "Xen: 137\n",
      "\n",
      "Removed 17543 duplicate entries\n",
      "Final total unique vulnerabilities: 1677\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def remove_duplicates():\n",
    "    DB_PATH = 'database.sqlite'\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # First, let's see how many duplicates we have\n",
    "    print(\"Checking for duplicates...\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT COMMIT_HASH, COUNT(*) as count\n",
    "        FROM vulnerabilities\n",
    "        GROUP BY COMMIT_HASH\n",
    "        HAVING COUNT(*) > 1\n",
    "    \"\"\")\n",
    "    duplicates = cursor.fetchall()\n",
    "    print(f\"Found {len(duplicates)} commit hashes with duplicates\")\n",
    "    \n",
    "    # Show some examples of duplicates\n",
    "    print(\"\\nExample duplicate entries:\")\n",
    "    for commit_hash, count in duplicates[:3]:  # Show first 3 examples\n",
    "        print(f\"COMMIT_HASH: {commit_hash}, Occurrences: {count}\")\n",
    "    \n",
    "    # Get initial count\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM vulnerabilities\")\n",
    "    initial_count = cursor.fetchone()[0]\n",
    "    print(f\"\\nInitial total entries: {initial_count}\")\n",
    "    \n",
    "    # Remove duplicates by keeping the first occurrence\n",
    "    cursor.execute(\"\"\"\n",
    "        DELETE FROM vulnerabilities \n",
    "        WHERE id NOT IN (\n",
    "            SELECT MIN(id)\n",
    "            FROM vulnerabilities\n",
    "            GROUP BY COMMIT_HASH\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Get count after removing duplicates\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM vulnerabilities\")\n",
    "    final_count = cursor.fetchone()[0]\n",
    "    \n",
    "    # Show counts by project\n",
    "    print(\"\\nCounts by project after removing duplicates:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT PROJECT, COUNT(*) as count\n",
    "        FROM vulnerabilities\n",
    "        GROUP BY PROJECT\n",
    "        ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    project_counts = cursor.fetchall()\n",
    "    for project, count in project_counts:\n",
    "        print(f\"{project.capitalize()}: {count}\")\n",
    "    \n",
    "    # Commit changes and close connection\n",
    "    conn.commit()\n",
    "    print(f\"\\nRemoved {initial_count - final_count} duplicate entries\")\n",
    "    print(f\"Final total unique vulnerabilities: {final_count}\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "remove_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GitInteraction:\n",
    "    def __init__(self, repo_path):\n",
    "        self.repo_path = repo_path\n",
    "        \n",
    "    def get_file_at_commit(self, commit_hash, file_path):\n",
    "        \"\"\"Get the contents of a file at a specific commit.\"\"\"\n",
    "        try:\n",
    "            command = [\"git\", \"show\", f\"{commit_hash}:{file_path}\"]\n",
    "            result = subprocess.run(command, cwd=self.repo_path, text=True, capture_output=True, check=True, encoding='utf-8', errors='ignore')\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error getting file at commit: {commit_hash} command: {command}\")\n",
    "            print(e.output)\n",
    "            return None\n",
    "        \n",
    "    def get_patch_of_commit(self, commit_hash, project):\n",
    "        \"\"\"Fetch the patch of a specific commit based on the project.\"\"\"\n",
    "        try:\n",
    "            if project == 'mozilla':\n",
    "                url = f\"https://github.com/mozilla/gecko-dev/commit/{commit_hash}.patch\"\n",
    "            elif project == 'xen':\n",
    "                url = f\"https://github.com/xen-project/xen/commit/{commit_hash}.patch\"\n",
    "            elif project == 'linux':\n",
    "                url = f\"https://github.com/torvalds/linux/commit/{commit_hash}.patch\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown project: {project}\")\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                patch_text = response.text\n",
    "                return patch_text\n",
    "            except requests.RequestException:\n",
    "                # Fallback to local git command if GitHub API fails\n",
    "                command = [\"git\", \"show\", commit_hash]\n",
    "                result = subprocess.run(command, cwd=self.repo_path, text=True, \n",
    "                                     capture_output=True, check=True, encoding='utf-8', \n",
    "                                     errors='ignore')\n",
    "                return result.stdout\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching patch for commit {commit_hash} in {project}\")\n",
    "            print(e)\n",
    "            return None\n",
    "        \n",
    "    def fetch_pre_fix_vulnerable_code(self, commit_hash, file_path):\n",
    "        \"\"\"Fetch vulnerable code segments from the commit prior to the fixing commit.\"\"\"\n",
    "        parent_commit_hash = f\"{commit_hash}^\"\n",
    "        return self.get_file_at_commit(parent_commit_hash, file_path)\n",
    "\n",
    "    def fetch_fixed_code(self, commit_hash, file_path):\n",
    "        \"\"\"Fetch patched code segments from the commit.\"\"\"\n",
    "        return self.get_file_at_commit(commit_hash, file_path)    \n",
    "    \n",
    "    def extract_function_signatures(self, code):\n",
    "        \"\"\"Extract function signatures from the code.\"\"\"\n",
    "        pattern = r'\\b(?:(?:static|struct\\s+\\w+\\s*\\*?)\\s+)*\\w+\\s+\\**\\w+\\s*\\([^)]*\\)\\s*\\{'\n",
    "        matches = re.findall(pattern, code, re.MULTILINE)\n",
    "        function_signatures = [match.strip() for match in matches]\n",
    "        return function_signatures\n",
    "    \n",
    "    def extract_files_and_functions_info(self, patch_text):\n",
    "        \"\"\"Extract the file paths and function names that contain added or deleted lines from a diff.\"\"\"\n",
    "        function_pattern = re.compile(r'^@@.*?@@\\s*(\\w[\\w\\s\\*]*)\\(')\n",
    "        file_path_pattern = re.compile(r'^diff --git a/(.*?) b/')\n",
    "\n",
    "        files_info = {}\n",
    "        current_function = None\n",
    "        current_file_path = None\n",
    "        current_added_block = []\n",
    "        current_deleted_block = []\n",
    "        lines = patch_text.split('\\n')\n",
    "\n",
    "        for line in lines:\n",
    "            file_match = file_path_pattern.search(line)\n",
    "            if file_match:\n",
    "                current_file_path = file_match.group(1).strip()\n",
    "                if current_file_path not in files_info:\n",
    "                    files_info[current_file_path] = {'functions': {}}\n",
    "                current_function = None  # Reset current function context when encountering a new file path\n",
    "                continue\n",
    "\n",
    "            match = function_pattern.search(line)\n",
    "            if match:\n",
    "                current_function = match.group(1).strip()\n",
    "                if current_function not in files_info[current_file_path]['functions']:\n",
    "                    files_info[current_file_path]['functions'][current_function] = {'added': [], 'deleted': []}\n",
    "                # Clear the current blocks when encountering a new function\n",
    "                current_added_block = []\n",
    "                current_deleted_block = []\n",
    "            else:\n",
    "                if current_file_path:\n",
    "                    if line.startswith('+') and not line.startswith('+++'):\n",
    "                        if current_deleted_block:\n",
    "                            if current_function:\n",
    "                                files_info[current_file_path]['functions'][current_function]['deleted'].append('\\n'.join(current_deleted_block))\n",
    "                            else:\n",
    "                                if 'deleted' not in files_info[current_file_path]:\n",
    "                                    files_info[current_file_path]['deleted'] = []\n",
    "                                files_info[current_file_path]['deleted'].append('\\n'.join(current_deleted_block))\n",
    "                            current_deleted_block = []\n",
    "                        current_added_block.append(line[1:].strip())\n",
    "                    elif line.startswith('-') and not line.startswith('---'):\n",
    "                        if current_added_block:\n",
    "                            if current_function:\n",
    "                                files_info[current_file_path]['functions'][current_function]['added'].append('\\n'.join(current_added_block))\n",
    "                            else:\n",
    "                                if 'added' not in files_info[current_file_path]:\n",
    "                                    files_info[current_file_path]['added'] = []\n",
    "                                files_info[current_file_path]['added'].append('\\n'.join(current_added_block))\n",
    "                            current_added_block = []\n",
    "                        current_deleted_block.append(line[1:].strip())\n",
    "                    else:\n",
    "                        if current_added_block:\n",
    "                            if current_function:\n",
    "                                files_info[current_file_path]['functions'][current_function]['added'].append('\\n'.join(current_added_block))\n",
    "                            else:\n",
    "                                if 'added' not in files_info[current_file_path]:\n",
    "                                    files_info[current_file_path]['added'] = []\n",
    "                                files_info[current_file_path]['added'].append('\\n'.join(current_added_block))\n",
    "                            current_added_block = []\n",
    "                        if current_deleted_block:\n",
    "                            if current_function:\n",
    "                                files_info[current_file_path]['functions'][current_function]['deleted'].append('\\n'.join(current_deleted_block))\n",
    "                            else:\n",
    "                                if 'deleted' not in files_info[current_file_path]:\n",
    "                                    files_info[current_file_path]['deleted'] = []\n",
    "                                files_info[current_file_path]['deleted'].append('\\n'.join(current_deleted_block))\n",
    "                            current_deleted_block = []\n",
    "\n",
    "        # Add any remaining blocks after the loop ends \n",
    "        if current_added_block:\n",
    "            if current_function:\n",
    "                files_info[current_file_path]['functions'][current_function]['added'].append('\\n'.join(current_added_block))\n",
    "            else:\n",
    "                if 'added' not in files_info[current_file_path]:\n",
    "                    files_info[current_file_path]['added'] = []\n",
    "                files_info[current_file_path]['added'].append('\\n'.join(current_added_block))\n",
    "        if current_deleted_block:\n",
    "            if current_function:\n",
    "                files_info[current_file_path]['functions'][current_function]['deleted'].append('\\n'.join(current_deleted_block))\n",
    "            else:\n",
    "                if 'deleted' not in files_info[current_file_path]:\n",
    "                    files_info[current_file_path]['deleted'] = []\n",
    "                files_info[current_file_path]['deleted'].append('\\n'.join(current_deleted_block))\n",
    "\n",
    "        # Remove empty strings from the added and deleted lines\n",
    "        for file_path, changes in files_info.items():\n",
    "            if 'added' in changes:\n",
    "                changes['added'] = list(filter(None, changes['added']))\n",
    "            if 'deleted' in changes:\n",
    "                changes['deleted'] = list(filter(None, changes['deleted']))\n",
    "            for function_name, function_changes in changes['functions'].items():\n",
    "                function_changes['added'] = list(filter(None, function_changes['added']))\n",
    "                function_changes['deleted'] = list(filter(None, function_changes['deleted']))\n",
    "                \n",
    "                # Remove empty string function names\n",
    "                if not function_name:\n",
    "                    del changes['functions'][function_name]\n",
    "\n",
    "        return files_info\n",
    "\n",
    "    def extract_function(self, code, function_name):\n",
    "        \"\"\" extract the entire vulnerable/patched function version of a specific function.\"\"\"\n",
    "        if not isinstance(code, str):\n",
    "            return None\n",
    "        \n",
    "        function_start_pattern = re.compile(r'\\b{}\\b\\s*\\([^{{}}]*\\)\\s*{{'.format(re.escape(function_name)), re.DOTALL)\n",
    "        match = function_start_pattern.search(code)\n",
    "        \n",
    "        if not match:\n",
    "            return None\n",
    "        \n",
    "        start_index = match.start()\n",
    "        \n",
    "        brace_stack = []\n",
    "        inside_function = False\n",
    "        end_index = start_index\n",
    "        \n",
    "        for i in range(start_index, len(code)):\n",
    "            if code[i] == '{':\n",
    "                brace_stack.append('{')\n",
    "                inside_function = True\n",
    "            elif code[i] == '}':\n",
    "                if brace_stack:\n",
    "                    brace_stack.pop()\n",
    "                    if not brace_stack:\n",
    "                        end_index = i + 1\n",
    "                        break\n",
    "        \n",
    "        if not inside_function or brace_stack:\n",
    "            return None\n",
    "        \n",
    "        function = code[start_index:end_index]\n",
    "        return function\n",
    "    \n",
    "    def is_change_within_function(self, function, changes):\n",
    "        \"\"\"Check if any change blocks are within the function.\"\"\"\n",
    "        function_lines = function.split('\\n')\n",
    "        change_blocks = changes['added'] + changes['deleted']\n",
    "    \n",
    "        for change in change_blocks:\n",
    "            change_lines = change.split('\\n')\n",
    "            change_lines = [line.strip() for line in change_lines if line.strip()]\n",
    "            \n",
    "            if not change_lines:\n",
    "                continue\n",
    "    \n",
    "            for i in range(len(function_lines) - len(change_lines) + 1):\n",
    "                match = True\n",
    "                for j in range(len(change_lines)):\n",
    "                    if change_lines[j] != function_lines[i + j].strip():\n",
    "                        match = False\n",
    "                        break\n",
    "                if match:\n",
    "                    return True\n",
    "        return False\n",
    "     \n",
    "    def parase_patch_header(self, patch_text):\n",
    "        \"\"\"Parse the patch header to extract the number of files changed, added, and deleted lines.\"\"\"\n",
    "        added_lines = 0\n",
    "        deleted_lines = 0\n",
    "        files_changed = set()\n",
    "        \n",
    "        file_pattern = re.compile(r'^diff --git a/(.*?) b/(.*?)$', re.MULTILINE)\n",
    "        # find all the files changed in the diff\n",
    "        matches = file_pattern.findall(patch_text)\n",
    "        for match in matches:\n",
    "            files_changed.add(match[0])\n",
    "        \n",
    "        # process each section starting with 'diff --git'\n",
    "        sections = re.split(r'(?m)^diff --git', patch_text)\n",
    "        for section in sections[1:]:  # Skip the first split as it's before the first 'diff --git'\n",
    "            lines = section.split('\\n')\n",
    "            for line in lines:\n",
    "                if line.startswith('+') and not line.startswith('+++'):\n",
    "                    added_lines += 1\n",
    "                elif line.startswith('-') and not line.startswith('---'):\n",
    "                    deleted_lines += 1\n",
    "        \n",
    "        return len(files_changed), added_lines, deleted_lines\n",
    "        \n",
    "    def extract_commit_description(self, commit_hash):\n",
    "        \"\"\"Extract the commit description.\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(['git', '-C', self.repo_path, 'log', '--format=%B', '-n', '1', commit_hash], stdout=subprocess.PIPE, text=True, encoding='utf-8')\n",
    "            description = result.stdout.strip()\n",
    "            return description\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error extracting description for commit {commit_hash}\")\n",
    "            print(e.output)\n",
    "            return None\n",
    "    \n",
    "    def build_code_blocks(self, files_info, commit_hash):\n",
    "        \"\"\"Build the vulnerable/patched code blocks from the extracted functions and added/deleted lines.\"\"\"\n",
    "        vulnerable_code_block = \"\"\n",
    "        patched_code_block = \"\"\n",
    "\n",
    "        # file level changes\n",
    "        for file_path, file_changes in files_info.items():\n",
    "            file_header_printed_vulnerable = False  # Flag to track the first entry (function or file-level change) in each file for vulnerable code\n",
    "            file_header_printed_patched = False  # Flag to track the first entry (function or file-level change) in each file for patched code\n",
    "\n",
    "            # Handle function-level changes\n",
    "            functions_to_modify = []\n",
    "            for function_name, changes in file_changes['functions'].items():\n",
    "                if not function_name:  # Skip empty string function names\n",
    "                    continue\n",
    "                vulnerable_code = self.fetch_pre_fix_vulnerable_code(commit_hash, file_path)\n",
    "                patched_code = self.fetch_fixed_code(commit_hash, file_path)\n",
    "\n",
    "                vulnerable_function = self.extract_function(vulnerable_code, function_name)\n",
    "                patched_function = self.extract_function(patched_code, function_name)\n",
    "\n",
    "                # Check if changes are within the function\n",
    "                if vulnerable_function and patched_function:\n",
    "                    changes_within_vulnerable_function = self.is_change_within_function(vulnerable_function, changes)\n",
    "                    changes_within_patched_function = self.is_change_within_function(patched_function, changes)\n",
    "\n",
    "                    if changes_within_vulnerable_function or changes_within_patched_function:\n",
    "                        \n",
    "                        if not file_header_printed_vulnerable:\n",
    "                            vulnerable_code_block += f\"// File path: {file_path}\\n\"\n",
    "                            file_header_printed_vulnerable = True\n",
    "                        if not file_header_printed_patched:\n",
    "                            patched_code_block += f\"// File path: {file_path}\\n\"\n",
    "                            file_header_printed_patched = True\n",
    "                        vulnerable_code_block += f\"{vulnerable_function}\\n\"\n",
    "                        patched_code_block += f\"{patched_function}\\n\"\n",
    "                    else:\n",
    "                        # added lines\n",
    "                        added_lines = '\\n'.join(changes['added'])\n",
    "                        deleted_lines = '\\n'.join(changes['deleted'])\n",
    "                        # General pattern for finding a pattern for function\n",
    "                        pattern = r'\\b([a-zA-Z_][a-zA-Z0-9_\\* ]*\\s+[a-zA-Z_][a-zA-Z0-9_]*)\\s*\\([^)]*\\)'\n",
    "                        \n",
    "                        # Check the function pattern in the added lines\n",
    "                        added_function_signatures = re.findall(pattern, added_lines, re.MULTILINE)\n",
    "                        deleted_function_signatures = re.findall(pattern, deleted_lines, re.MULTILINE)\n",
    "                        \n",
    "                        # if find any pattern extract the function name and modify the function name (functions)\n",
    "                        if added_function_signatures or deleted_function_signatures:\n",
    "                            new_function_name = added_function_signatures[0] if added_function_signatures else deleted_function_signatures[0]\n",
    "                            functions_to_modify.append((function_name, new_function_name))\n",
    "                        else:\n",
    "                            functions_to_modify.append((function_name, \"\"))\n",
    "                            \n",
    "                else:\n",
    "                    if 'added' in changes and changes['added']: # if added lines contain any function signature, then fetch the entire function\n",
    "                        function_signatures = self.extract_function_signatures('\\n'.join(changes['added']))\n",
    "                        if function_signatures:\n",
    "                            \n",
    "                            new_function_name = function_signatures[0]\n",
    "                            functions_to_modify.append((function_name, new_function_name))\n",
    "                            patched_function = self.extract_function(patched_code, new_function_name)\n",
    "                        else:\n",
    "                            # append the added lines to the patched code block\n",
    "                            patched_function = '\\n'.join(changes['added'])\n",
    "                            functions_to_modify.append((function_name, \"\"))\n",
    "                           \n",
    "                        if not file_header_printed_patched:\n",
    "                            patched_code_block += f\"// File path: {file_path}\\n\"\n",
    "                            file_header_printed_patched = True\n",
    "                        patched_code_block += f\"{patched_function}\\n\"\n",
    "\n",
    "                    if 'deleted' in changes and changes['deleted']: # if any deleted lines in the function, then fetch the entire function\n",
    "                        \n",
    "                        function_signatures = self.extract_function_signatures('\\n'.join(changes['deleted']))\n",
    "                        if function_signatures:\n",
    "                            \n",
    "                            new_function_name = function_signatures[0]\n",
    "                            functions_to_modify.append((function_name, new_function_name))\n",
    "                            vulnerable_function = self.extract_function(vulnerable_code, new_function_name)\n",
    "                        else:\n",
    "                            # append the deleted lines to the vulnerable code block\n",
    "                            # vulnerable_code_block += f\"{''.join(changes['deleted'])}\\n\"\n",
    "                            vulnerable_function = '\\n'.join(changes['deleted'])\n",
    "                            if function_name in functions_to_modify:\n",
    "                                continue\n",
    "                            else:\n",
    "                                functions_to_modify.append((function_name, \"\"))\n",
    "\n",
    "                        if not file_header_printed_vulnerable:\n",
    "                            vulnerable_code_block += f\"// File path: {file_path}\\n\"\n",
    "                            file_header_printed_vulnerable = True\n",
    "                        vulnerable_code_block += f\"{vulnerable_function}\\n\"\n",
    "            \n",
    "            # Add new functions after iteration\n",
    "            functions_to_modify = list(set(functions_to_modify))  # Remove duplicates\n",
    "            for function_name, new_function_name in functions_to_modify:\n",
    "                if not function_name:  # Skip empty string function names\n",
    "                    continue\n",
    "                # if a function name is already in the functions, first combine the added and deleted lines and then delete the original function name\n",
    "                if new_function_name in files_info[file_path]['functions']:\n",
    "                    # Combine the added and deleted lines\n",
    "                    combine_add = files_info[file_path]['functions'][function_name]['added'] + files_info[file_path]['functions'][new_function_name]['added']\n",
    "                    combine_del = files_info[file_path]['functions'][function_name]['deleted'] + files_info[file_path]['functions'][new_function_name]['deleted']\n",
    "                    # Assign the combined lines to the new function name\n",
    "                    files_info[file_path]['functions'][new_function_name] = {'added': combine_add, 'deleted': combine_del}\n",
    "                    # Delete the original function name\n",
    "                    del files_info[file_path]['functions'][function_name]\n",
    "                else:\n",
    "                    # Extract the value associated with the original key\n",
    "                    original_value = files_info[file_path]['functions'][function_name]\n",
    "                    # Delete the original function name\n",
    "                    del files_info[file_path]['functions'][function_name]\n",
    "                    # Assign the extracted value to the new key\n",
    "                    files_info[file_path]['functions'][new_function_name] = original_value\n",
    "                # print(f\"function_name: {function_name} new_function_name: {new_function_name}\")\n",
    "                \n",
    "                # Re-extract and re-process the modified function\n",
    "                # 1. skip empty string function name\n",
    "                # 2. skip if the function name is already in the functions\n",
    "                if not new_function_name:\n",
    "                    continue\n",
    "                # if a function name is already added to the vulnerable_code_block or patched_code_block, then skip\n",
    "                if new_function_name in vulnerable_code_block or new_function_name in patched_code_block:\n",
    "                    continue\n",
    "                vulnerable_code = self.fetch_pre_fix_vulnerable_code(commit_hash, file_path)\n",
    "                patched_code = self.fetch_fixed_code(commit_hash, file_path)\n",
    "    \n",
    "                vulnerable_function = self.extract_function(vulnerable_code, new_function_name)\n",
    "                patched_function = self.extract_function(patched_code, new_function_name)\n",
    "                # print(f\"vulnerable_function: {vulnerable_function}\\n patched_function: {patched_function}\\n\")\n",
    "                if vulnerable_function or patched_function:\n",
    "                    if not file_header_printed_vulnerable:\n",
    "                        vulnerable_code_block += f\"// File path: {file_path}\\n\"\n",
    "                        file_header_printed_vulnerable = True\n",
    "                    if not file_header_printed_patched:\n",
    "                        patched_code_block += f\"// File path: {file_path}\\n\"\n",
    "                        file_header_printed_patched = True\n",
    "                    vulnerable_code_block += f\"{vulnerable_function}\\n\"\n",
    "                    patched_code_block += f\"{patched_function}\\n\"\n",
    "            \n",
    "            \n",
    "            # Handle file-level changes\n",
    "            if 'added' in file_changes and file_changes['added']:\n",
    "                if not file_header_printed_patched:\n",
    "                    patched_code_block += f\"// File path: {file_path}\\n\"\n",
    "                    file_header_printed_patched = True\n",
    "                patched_code_block += f\"{''.join(file_changes['added'])}\\n\"\n",
    "\n",
    "            if 'deleted' in file_changes and file_changes['deleted']:\n",
    "                if not file_header_printed_vulnerable:\n",
    "                    vulnerable_code_block += f\"// File path: {file_path}\\n\"\n",
    "                    file_header_printed_vulnerable = True\n",
    "                vulnerable_code_block += f\"{''.join(file_changes['deleted'])}\\n\"\n",
    "\n",
    "        return files_info, vulnerable_code_block, patched_code_block\n",
    "        \n",
    "    def num_functions_changed(self, vulnerable_code_block, patched_code_block):\n",
    "        \"\"\"Calculate the number of functions changed between the vulnerable and patched code blocks.\"\"\"\n",
    "        vulnerable_functions = self.extract_function_signatures(vulnerable_code_block)\n",
    "        patched_functions = self.extract_function_signatures(patched_code_block)\n",
    "        unique_functions = set(vulnerable_functions + patched_functions)\n",
    "        \n",
    "        return len(unique_functions)\n",
    "    \n",
    "    def save_code_blocks(self, conn, cursor, commit_hash, vulnerable_code_block, patched_code_block, project):\n",
    "        \"\"\"Save the code blocks and metadata to the database.\"\"\"\n",
    "        num_files_changed, num_lines_added, num_lines_deleted = self.parase_patch_header(\n",
    "            self.get_patch_of_commit(commit_hash, project))\n",
    "        num_functions_changed = self.num_functions_changed(vulnerable_code_block, patched_code_block)\n",
    "        commit_description = self.extract_commit_description(commit_hash)\n",
    "\n",
    "        cursor.execute('''\n",
    "        UPDATE vulnerabilities\n",
    "        SET DESCRIPTION_IN_PATCH = ?,\n",
    "            VULNERABLE_CODE_BLOCK = ?,\n",
    "            PATCHED_CODE_BLOCK = ?,\n",
    "            NUM_FILES_CHANGED = ?,\n",
    "            NUM_FUNCTIONS_CHANGED = ?,\n",
    "            NUM_LINES_ADDED = ?,\n",
    "            NUM_LINES_DELETED = ?\n",
    "        WHERE COMMIT_HASH = ? AND PROJECT = ?\n",
    "        ''', (commit_description, vulnerable_code_block, patched_code_block, \n",
    "              num_files_changed, num_functions_changed, num_lines_added, \n",
    "              num_lines_deleted, commit_hash, project))\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Processing complete for commit {commit_hash} in {project}.\")\n",
    "\n",
    "# Modified process_commit function to include project\n",
    "def process_commit(git_interaction, db_path, commit_hash, project):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    patch_text = git_interaction.get_patch_of_commit(commit_hash, project)\n",
    "    if patch_text:\n",
    "        info = git_interaction.extract_files_and_functions_info(patch_text)\n",
    "        file_info, vulnerable_code_block, patched_code_block = git_interaction.build_code_blocks(info, commit_hash)\n",
    "        git_interaction.save_code_blocks(conn, cursor, commit_hash, vulnerable_code_block, patched_code_block, project)\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    # Read and process the dataset\n",
    "    raw_data = pd.read_csv('raw-dataset-merged-class-20211022013122.csv')\n",
    "    \n",
    "    # Process each project\n",
    "    project_data = {\n",
    "        'mozilla': raw_data[raw_data['R_ID'] == 1],\n",
    "        'xen': raw_data[raw_data['R_ID'] == 3],\n",
    "        'linux': raw_data[raw_data['R_ID'] == 2]\n",
    "    }\n",
    "\n",
    "    # Create database connection for main function\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        for project, data in project_data.items():\n",
    "            git_interaction = GitInteraction(REPO_PATHS[project])\n",
    "            commits = data['COMMIT_HASH'].tolist()\n",
    "            \n",
    "            print(f\"Processing {project} data: {len(commits)} commits\")\n",
    "            \n",
    "            # Process commits in parallel\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = [\n",
    "                    executor.submit(process_commit, git_interaction, DB_PATH, commit, project) \n",
    "                    for commit in commits\n",
    "                ]\n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        future.result()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing commit in {project}: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting code block line count update...\n",
      "Checking for required columns...\n",
      "Found 1677 records to process\n",
      "Processed 100/1677 records...\n",
      "Processed 200/1677 records...\n",
      "Processed 300/1677 records...\n",
      "Processed 400/1677 records...\n",
      "Processed 500/1677 records...\n",
      "Processed 600/1677 records...\n",
      "Processed 700/1677 records...\n",
      "Processed 800/1677 records...\n",
      "Processed 900/1677 records...\n",
      "Processed 1000/1677 records...\n",
      "Processed 1100/1677 records...\n",
      "Processed 1200/1677 records...\n",
      "Processed 1300/1677 records...\n",
      "Processed 1400/1677 records...\n",
      "Processed 1500/1677 records...\n",
      "Processed 1600/1677 records...\n",
      "Processed 1677/1677 records...\n",
      "\n",
      "Update completed!\n",
      "Total records processed: 1677\n",
      "\n",
      "Statistics by project:\n",
      "linux:\n",
      "  Records: 363\n",
      "  Average vulnerable lines: 178.69\n",
      "  Average patched lines: 184.17\n",
      "mozilla:\n",
      "  Records: 1177\n",
      "  Average vulnerable lines: 85.28\n",
      "  Average patched lines: 242.66\n",
      "xen:\n",
      "  Records: 137\n",
      "  Average vulnerable lines: 403.64\n",
      "  Average patched lines: 422.12\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def get_db_connection(db_file, max_retries=5, retry_delay=1):\n",
    "    \"\"\"Create a database connection with retry logic.\"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_file, timeout=20)  # Increased timeout\n",
    "            conn.execute('PRAGMA journal_mode=WAL')  # Use WAL mode for better concurrency\n",
    "            conn.execute('PRAGMA busy_timeout=30000')  # Set busy timeout to 30 seconds\n",
    "            cursor = conn.cursor()\n",
    "            yield conn, cursor\n",
    "            return\n",
    "        except sqlite3.OperationalError as e:\n",
    "            attempt += 1\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "            print(f\"Database is locked, retrying in {retry_delay} seconds... (Attempt {attempt}/{max_retries})\")\n",
    "            time.sleep(retry_delay)\n",
    "        finally:\n",
    "            if 'conn' in locals():\n",
    "                try:\n",
    "                    conn.close()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "def count_lines(code_block):\n",
    "    \"\"\"Count number of lines in a code block, handling None values.\"\"\"\n",
    "    if code_block is None:\n",
    "        return 0\n",
    "    return len(code_block.split('\\n'))\n",
    "\n",
    "def update_num_lines_in_code_blocks(db_file='database.sqlite'):\n",
    "    \"\"\"Update the number of lines in vulnerable and patched code blocks.\"\"\"\n",
    "    print(\"Starting code block line count update...\")\n",
    "    \n",
    "    try:\n",
    "        with get_db_connection(db_file) as (conn, cursor):\n",
    "            # Check if the columns exist\n",
    "            print(\"Checking for required columns...\")\n",
    "            cursor.execute(\"PRAGMA table_info(vulnerabilities)\")\n",
    "            columns = [info[1] for info in cursor.fetchall()]\n",
    "            \n",
    "            # Add columns if they don't exist\n",
    "            if 'NUM_LINES_IN_VULNERABLE_CODE_BLOCK' not in columns:\n",
    "                print(\"Adding vulnerable code block line count column...\")\n",
    "                cursor.execute(\"ALTER TABLE vulnerabilities ADD COLUMN NUM_LINES_IN_VULNERABLE_CODE_BLOCK INTEGER\")\n",
    "            \n",
    "            if 'NUM_LINES_IN_PATCHED_CODE_BLOCK' not in columns:\n",
    "                print(\"Adding patched code block line count column...\")\n",
    "                cursor.execute(\"ALTER TABLE vulnerabilities ADD COLUMN NUM_LINES_IN_PATCHED_CODE_BLOCK INTEGER\")\n",
    "\n",
    "            # First, get total count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM vulnerabilities\")\n",
    "            total_rows = cursor.fetchone()[0]\n",
    "            print(f\"Found {total_rows} records to process\")\n",
    "\n",
    "            # Process in batches to avoid memory issues\n",
    "            batch_size = 100\n",
    "            offset = 0\n",
    "            updated_count = 0\n",
    "\n",
    "            while offset < total_rows:\n",
    "                # Fetch batch of records\n",
    "                cursor.execute(\"\"\"\n",
    "                    SELECT COMMIT_HASH, VULNERABLE_CODE_BLOCK, PATCHED_CODE_BLOCK, PROJECT \n",
    "                    FROM vulnerabilities LIMIT ? OFFSET ?\n",
    "                \"\"\", (batch_size, offset))\n",
    "                \n",
    "                batch_rows = cursor.fetchall()\n",
    "                if not batch_rows:\n",
    "                    break\n",
    "\n",
    "                # Process batch\n",
    "                for row in batch_rows:\n",
    "                    commit_hash, vulnerable_code_block, patched_code_block, project = row\n",
    "                    num_lines_vulnerable = count_lines(vulnerable_code_block)\n",
    "                    num_lines_patched = count_lines(patched_code_block)\n",
    "                    \n",
    "                    cursor.execute(\"\"\"\n",
    "                        UPDATE vulnerabilities\n",
    "                        SET NUM_LINES_IN_VULNERABLE_CODE_BLOCK = ?,\n",
    "                            NUM_LINES_IN_PATCHED_CODE_BLOCK = ?\n",
    "                        WHERE COMMIT_HASH = ? AND PROJECT = ?\n",
    "                    \"\"\", (num_lines_vulnerable, num_lines_patched, commit_hash, project))\n",
    "                    \n",
    "                    updated_count += 1\n",
    "\n",
    "                conn.commit()  # Commit each batch\n",
    "                print(f\"Processed {updated_count}/{total_rows} records...\")\n",
    "                offset += batch_size\n",
    "\n",
    "            print(\"\\nUpdate completed!\")\n",
    "            print(f\"Total records processed: {updated_count}\")\n",
    "            \n",
    "            # Get statistics by project\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT PROJECT, \n",
    "                       AVG(NUM_LINES_IN_VULNERABLE_CODE_BLOCK) as avg_vuln_lines,\n",
    "                       AVG(NUM_LINES_IN_PATCHED_CODE_BLOCK) as avg_patch_lines,\n",
    "                       COUNT(*) as count\n",
    "                FROM vulnerabilities\n",
    "                GROUP BY PROJECT\n",
    "            \"\"\")\n",
    "            \n",
    "            print(\"\\nStatistics by project:\")\n",
    "            for project, avg_vuln, avg_patch, count in cursor.fetchall():\n",
    "                print(f\"{project}:\")\n",
    "                print(f\"  Records: {count}\")\n",
    "                print(f\"  Average vulnerable lines: {avg_vuln:.2f}\")\n",
    "                print(f\"  Average patched lines: {avg_patch:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating line counts: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    update_num_lines_in_code_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique CWE count after removal: 52\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to the database\n",
    "db_path = 'database.sqlite'\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Find samples where only one of the two conditions is met (to identify the 89 samples)\n",
    "exclusive_samples_query = \"\"\"\n",
    "    SELECT id FROM vulnerabilities\n",
    "    WHERE (NUM_LINES_IN_VULNERABLE_CODE_BLOCK > 500 OR NUM_LINES_IN_PATCHED_CODE_BLOCK > 500)\n",
    "\"\"\"\n",
    "cursor.execute(exclusive_samples_query)\n",
    "exclusive_sample_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Remove these exclusive samples from the dataset\n",
    "if exclusive_sample_ids:\n",
    "    cursor.execute(f\"DELETE FROM vulnerabilities WHERE id IN ({','.join(map(str, exclusive_sample_ids))})\")\n",
    "    conn.commit()\n",
    "\n",
    "# Count unique CWE values after removal\n",
    "cursor.execute(\"SELECT COUNT(DISTINCT VULNERABILITY_CWE) FROM vulnerabilities\")\n",
    "unique_cwe_after_removal = cursor.fetchone()[0]\n",
    "\n",
    "print(f\"Unique CWE count after removal: {unique_cwe_after_removal}\")\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after deletion: 986\n",
      "Unique CWE count after deletion: 53\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to the database\n",
    "db_path = 'database.sqlite'\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Find samples where VULNERABILITY_CWE, VULNERABLE_CODE_BLOCK, or PATCHED_CODE_BLOCK are NULL or empty\n",
    "exclusive_samples_query = \"\"\"\n",
    "    SELECT id FROM vulnerabilities\n",
    "    WHERE VULNERABILITY_CWE IS NULL OR VULNERABILITY_CWE = ''\n",
    "       OR VULNERABLE_CODE_BLOCK IS NULL OR VULNERABLE_CODE_BLOCK = ''\n",
    "       OR PATCHED_CODE_BLOCK IS NULL OR PATCHED_CODE_BLOCK = ''\n",
    "\"\"\"\n",
    "cursor.execute(exclusive_samples_query)\n",
    "exclusive_sample_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Remove these exclusive samples from the dataset\n",
    "if exclusive_sample_ids:\n",
    "    cursor.execute(f\"DELETE FROM vulnerabilities WHERE id IN ({','.join(map(str, exclusive_sample_ids))})\")\n",
    "    conn.commit()\n",
    "\n",
    "# Count total samples after deletion\n",
    "cursor.execute(\"SELECT COUNT(*) FROM vulnerabilities\")\n",
    "total_samples_after_full_clean = cursor.fetchone()[0]\n",
    "\n",
    "# Count unique CWE values after deletion\n",
    "cursor.execute(\"SELECT COUNT(DISTINCT VULNERABILITY_CWE) FROM vulnerabilities\")\n",
    "unique_cwe_after_full_clean = cursor.fetchone()[0]\n",
    "\n",
    "print(f\"Total samples after deletion: {total_samples_after_full_clean}\")\n",
    "print(f\"Unique CWE count after deletion: {unique_cwe_after_full_clean}\")\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1,677 rows from 'vulnerabilities'...\n",
      "Successfully exported to 'vulnerabilities.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "def export_table_to_csv(\n",
    "    db_path: Union[str, Path], \n",
    "    table_name: str, \n",
    "    csv_path: Union[str, Path],\n",
    "    encoding: str = 'utf-8'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Export SQLite table to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        table_name: Name of table to export\n",
    "        csv_path: Output CSV file path\n",
    "        encoding: CSV file encoding\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    db_path = Path(db_path)\n",
    "    csv_path = Path(csv_path)\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        raise FileNotFoundError(f\"Database file '{db_path}' not found\")\n",
    "    \n",
    "    if not table_name:\n",
    "        raise ValueError(\"Table name cannot be empty\")\n",
    "\n",
    "    try:\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Get column names to preserve schema order\n",
    "            cursor.execute(f'PRAGMA table_info(\"{table_name}\")')\n",
    "            columns_info = cursor.fetchall()\n",
    "            if not columns_info:\n",
    "                raise ValueError(f\"Table '{table_name}' does not exist\")\n",
    "            \n",
    "            column_names = [info[1] for info in columns_info]\n",
    "            \n",
    "            # Get row count for progress\n",
    "            cursor.execute(f'SELECT COUNT(*) FROM \"{table_name}\"')\n",
    "            total_rows = cursor.fetchone()[0]\n",
    "            print(f\"Exporting {total_rows:,} rows from '{table_name}'...\")\n",
    "\n",
    "            # Corrected query construction without escaping issues\n",
    "            column_names_escaped = ', '.join(f'\"{col}\"' for col in column_names)  # Ensure proper escaping\n",
    "            query = f'SELECT {column_names_escaped} FROM \"{table_name}\"'\n",
    "\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            df = df[column_names]  # Ensure correct column order\n",
    "            \n",
    "            df.to_csv(csv_path, index=False, quoting=csv.QUOTE_MINIMAL, encoding=encoding)\n",
    "            print(f\"Successfully exported to '{csv_path}'\")\n",
    "    \n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"SQLite error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Parameters\n",
    "db_path = 'database.sqlite'\n",
    "table_name = 'vulnerabilities'\n",
    "csv_path = 'vulnerabilities.csv'\n",
    "\n",
    "export_table_to_csv(db_path, table_name, csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmserver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
