# IMPORTANT: These must be the first lines in your script
import os
import tempfile
import time
from functools import wraps

# Disable all caching
os.environ['LITELLM_SKIP_CACHE'] = "true"
os.environ['LITELLM_CACHE_DISABLE'] = "true"
os.environ['NO_CACHE'] = "true"
os.environ['DISABLE_CACHE'] = "true"

# Import patch first
try:
    import patch
except ImportError:
    print("Warning: Could not import patch module, continuing without it...")

# Now import the rest
import sqlite3
import dspy
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import torch

# Configure GPU settings
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    device = "cuda"
else:
    device = "cpu"

# Configure DSPy with optimized settings and no cache
lm = dspy.LM("ollama_chat/deepseek-r1",
             api_base="http://localhost:11434",
             api_key="local", 
             model_type='chat',
             cache=None,
             device=device)

# Force disable caching
dspy.settings.configure(cache=None, disable_cache=True)
dspy.configure(lm=lm)

class GetBasicAnswer(dspy.Signature):
    """Answer questions with short factoid answers."""
    question = dspy.InputField()
    reasoning = dspy.OutputField(desc="Explain your reasoning for the final decision on the vulnerability assessment.")
    answer = dspy.OutputField(desc="""
        Analyze the reasoning text and determine the vulnerability status.
        Return exactly one of these values:
        - 1: if the text clearly indicates a vulnerability is present
        - 0: if the text clearly indicates no vulnerability is present
        - 2: if the text is ambiguous or requires more information
    """)

def retry_on_connection_error(max_retries=3, delay=5):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if "Connection refused" in str(e):
                        retries += 1
                        if retries < max_retries:
                            print(f"\nConnection error. Retrying in {delay} seconds... (Attempt {retries}/{max_retries})")
                            time.sleep(delay)
                        else:
                            print("\nMax retries reached. Returning default value.")
                            return 2  # Return ambiguous result after max retries
                    else:
                        raise e
            return 2  # Return ambiguous result if all retries fail
        return wrapper
    return decorator

@retry_on_connection_error(max_retries=3, delay=5)
def process_batch(batch_data, answer_chain):
    """Process a batch of rows in parallel"""
    results = []
    questions = [row[2] for row in batch_data if row[2]]  # Extract reasoning texts
    
    if not questions:
        return [(row[0], row[1], 2) for row in batch_data]
    
    try:
        # Process all questions in one batch
        predictions = [answer_chain(question=q) for q in questions]
        
        # Process results
        q_idx = 0
        for row in batch_data:
            rowid, commit_hash, reasoning_text = row
            if reasoning_text:
                try:
                    stripped = predictions[q_idx].answer.strip()
                    decision = int(stripped[0]) if stripped and stripped[0] in ['0', '1', '2'] else 2
                    q_idx += 1
                except Exception as e:
                    print(f"Error processing COMMIT_HASH {commit_hash}: {str(e)}")
                    decision = 2
            else:
                decision = 2
            results.append((rowid, commit_hash, decision))
            
    except Exception as e:
        print(f"Batch processing error: {str(e)}")
        results.extend([(row[0], row[1], 2) for row in batch_data])
    
    return results

def process_database(start_column, 
                    db_path="/users/azibaeir/Research/Benchmarking/project/vulnerability_dataset/database/database.sqlite", 
                    table_name="vulnerabilities_codellama_7b",
                    batch_size=32,  # Increased batch size for GPU efficiency
                    num_workers=4): # Number of parallel workers
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    columns = [
        ("COT_REASONING_VULN", "COT_VULN"),
        ("COT_REASONING_PATCH", "COT_PATCH"),
        ("THink_REASONING_VULN", "THink_VULN"),
        ("THink_REASONING_PATCH", "THink_PATCH"),
        ("THINK_REASONING_VULN", "THINK_VULN"),
        ("THINK_REASONING_PATCH", "THINK_PATCH")
    ]
    
    # Find starting point
    start_idx = next((i for i, (col, _) in enumerate(columns) if col == start_column), 0)
    columns = columns[start_idx:]
    
    # Initialize chain-of-thought module
    answer_chain = dspy.ChainOfThought(GetBasicAnswer)
    
    try:
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            for reasoning_col, decision_col in columns:
                cursor.execute(f"SELECT rowid, COMMIT_HASH, {reasoning_col} FROM {table_name}")
                all_rows = cursor.fetchall()
                
                # Process in batches with progress bar
                with tqdm(total=len(all_rows), desc=f"Processing {reasoning_col}") as pbar:
                    for batch_start in range(0, len(all_rows), batch_size):
                        batch = all_rows[batch_start:batch_start + batch_size]
                        
                        # Process batch
                        results = process_batch(batch, answer_chain)
                        
                        # Update database
                        for rowid, commit_hash, decision in results:
                            cursor.execute(
                                f"UPDATE {table_name} SET {decision_col} = ? WHERE rowid = ?",
                                (decision, rowid)
                            )
                            print(f"COMMIT_HASH: {commit_hash} | Decision: {decision}")
                        
                        conn.commit()
                        pbar.update(len(batch))
                        
                print(f"\nCompleted processing {reasoning_col}\n")
    
    finally:
        conn.close()

if __name__ == "__main__":
    # Adjust these parameters based on your GPU
    process_database(
        start_column="COT_REASONING_VULN",
        batch_size=64,  # Increased for better GPU utilization
        num_workers=8   # Increased for better CPU utilization
    )
    