# Vulnerability Dataset Processor

A modular Python tool for processing and analyzing vulnerability datasets from Mozilla, Linux, and Xen repositories. This tool extracts vulnerability information, code changes, and provides detailed analysis of security patches.

## Project Structure

```
vulnerability_dataset/
├── config/
│   ├── __init__.py
│   └── settings.py           # Configuration settings
├── database/
│   ├── __init__.py
│   ├── db_manager.py        # Database operations
│   └── db_utils.py          # Database utilities
├── git/
│   ├── __init__.py
│   ├── git_manager.py       # Git operations
│   └── git_utils.py         # Git utilities
├── data_processing/
│   ├── __init__.py
│   ├── cleaner.py           # Data cleaning operations
│   └── exporter.py          # Data export operations
├── utils/
│   ├── __init__.py
│   └── logger.py            # Logging utilities
└── main.py                  # Main entry point
```

## Prerequisites

- Python 3.8 or higher
- Git
- Local clones of the following repositories:
  - Mozilla: [gecko-dev](https://github.com/mozilla/gecko-dev)
  - Linux: [linux](https://github.com/torvalds/linux)
  - Xen: [xen](https://github.com/xen-project/xen)

## Installation

1. Clone this repository:
```bash
git clone <repository-url>
cd vulnerability_dataset
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install required packages:
```bash
pip install -r requirements.txt
```

4. Set up repository paths in `config/settings.py`:
```python
REPO_PATHS = {
    'mozilla': '/path/to/gecko-dev',
    'xen': '/path/to/xen',
    'linux': '/path/to/linux'
}
```

## Usage

1. Basic usage:
```bash
python main.py
```

2. Run specific operations:
```python
from database import DatabaseManager
from git import GitManager
from data_processing.cleaner import DataCleaner
from data_processing.exporter import DataExporter

# Initialize components
db_manager = DatabaseManager()
git_manager = GitManager()
data_cleaner = DataCleaner()
data_exporter = DataExporter()

# Create and process database
db_manager.create_database()
data_cleaner.remove_duplicates()
data_cleaner.remove_invalid_entries()
data_exporter.export_to_csv('vulnerabilities.csv')
```

## Features

### Database Operations
- Create SQLite database from raw dataset
- Store vulnerability information
- Track code changes and patches
- Maintain metadata about vulnerabilities

### Git Operations
- Extract patches from repositories
- Analyze code changes
- Track vulnerable and patched code versions
- Extract commit metadata

### Data Processing
- Remove duplicate entries
- Clean invalid or incomplete data
- Export data to CSV format
- Generate statistics and analysis

### Data Analysis Capabilities
- Count lines of code in vulnerable/patched versions
- Track number of files and functions changed
- Analyze CWE distributions
- Generate project-specific statistics

## Configuration

Key settings in `config/settings.py`:

```python
# Database settings
DB_PATH = 'path/to/database.sqlite'
DB_TIMEOUT = 20
DB_JOURNAL_MODE = "WAL"
DB_BUSY_TIMEOUT = 30000

# Repository paths
REPO_PATHS = {
    'mozilla': '/path/to/gecko-dev',
    'xen': '/path/to/xen',
    'linux': '/path/to/linux'
}

# Raw dataset path
RAW_DATASET_PATH = 'path/to/raw-dataset.csv'
```

## Input Data Format

The input CSV should contain the following columns:
- COMMIT_HASH
- VULNERABILITY_CVE
- VULNERABILITY_YEAR
- VULNERABILITY_CWE
- VULNERABILITY_CATEGORY
- R_ID (1 for Mozilla, 2 for Linux, 3 for Xen)

## Output Format

The tool generates:
1. SQLite database with detailed vulnerability information
2. CSV export with processed data
3. Statistical analysis of vulnerabilities

## Error Handling

- All operations are logged to both console and file
- Comprehensive error handling for Git operations
- Database connection retry mechanism
- Invalid data handling and cleaning

## Performance

- Uses SQLite WAL mode for better concurrency
- Implements batch processing for large datasets
- Uses ThreadPoolExecutor for parallel processing
- Optimized git operations with caching

## Troubleshooting

Common issues and solutions:

1. Database locked:
   - Increase `DB_TIMEOUT` in settings
   - Check for other processes using the database

2. Git repository issues:
   - Ensure repositories are properly cloned
   - Update repository paths in settings
   - Check git installation

3. Memory issues with large datasets:
   - Adjust batch size in data processing
   - Use smaller chunks for processing

## Contributing

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a new Pull Request
